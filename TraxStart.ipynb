{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "TraxStart.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrjd9WTmDMwdZXQvzIG/Pa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sirsirious/trax_start/blob/colabs/TraxStart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ_VJUymklkb"
      },
      "source": [
        "\n",
        "# An introduction to Trax\n",
        "\n",
        "Trax is an amazing Deep Learning library that's being developed and maintained by **Google Brain Team**.\n",
        "\n",
        "It combines the power and experience of Tensorflow with a cleaner, easier to understand code, in a way that resembles a lot the Keras library (which is a part of tensorflow itself now).\n",
        "\n",
        "Since it's being developed by Google Brain Team, it inclueds a lot of state-of-the-art algorithms and techniques, especially in *Natural Language Processing*!\n",
        "\n",
        "For example, it contains code for:\n",
        "\n",
        "\n",
        "*   Seq2Seq Models\n",
        "*   Transformer Models\n",
        "*   BERt Model\n",
        "*   Reformer Model (the efficient Transformer!)\n",
        "\n",
        "Aside from this, there are many useful preprocessing tools that help you get your NLP Training running as fast as a bolt!\n",
        "\n",
        "Let us get started!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c3XdVoux7WA"
      },
      "source": [
        "# Run this cell to set TPU in Colab\r\n",
        "import os\r\n",
        "import jax\r\n",
        "import requests\r\n",
        "# Run this to get the TPU address.\r\n",
        "if 'TPU_DRIVER_MODE' not in globals():\r\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\r\n",
        "  resp = requests.post(url)\r\n",
        "  TPU_DRIVER_MODE = 1\r\n",
        "\r\n",
        "# The following is required to use TPU Driver as JAX's backend.\r\n",
        "from jax.config import config\r\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\r\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMEIaC-eyPWg"
      },
      "source": [
        "try:\n",
        "    import trax\n",
        "except ModuleNotFoundError:\n",
        "    # Woa google, didn't you get your own library to your colabs yet!?\n",
        "    !pip install trax\n",
        "    import trax\n",
        "    \n",
        "trax.fastmath.set_backend('jax')\n",
        "# Layers used to build the Deep Learning Models\n",
        "import trax.layers as tl\n",
        "# Utilities to build and download datasets\n",
        "import trax.data as data\n",
        "# Fastmath is trax interface to numpy, or jax, or tensorflow-numpy. Jax has some numpy operations working much faster, but not all of them, so here's the need for both.\n",
        "import trax.fastmath as np\n",
        "# Basics for training\n",
        "import trax.supervised as ts\n",
        "\n",
        "import numpy # default numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUcJ2ATEnRUH"
      },
      "source": [
        "\n",
        "## Preprocessing Pipelines\n",
        "---\n",
        "\n",
        "Trax provides us easy to use resources to implement NLP Pipelines (actually, any preprocessing pipeline).\n",
        "\n",
        "For example, Trax allows us to use many Tensorflow utilities, such as Tensorflow Datasets. We can download any dataset available there very easily.\n",
        "\n",
        "To do it, we use:\n",
        "\n",
        "```python \n",
        "data.TFDS(dataset_name, keys=(which_value_field_to_get, target_value), train=if_data_is_train_or_eval)\n",
        "```\n",
        "\n",
        "We can, for example, get the 'imdb_reviews' dataset to be used for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0SULfTLy0ab"
      },
      "source": [
        "# Get dataset\n",
        "imdb = data.TFDS('imdb_reviews', keys=('text', 'label'), train=True) #This returns a function that wraps a generator."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ_0si2poP29"
      },
      "source": [
        "# We can check the next value by using next(dataset). It returns the text in byte format and the sentiment (0 = neg, 1 = pos).\n",
        "# Since it is a function, next(dataset_func()) will always return the first sentence in the dataset, unless we get the result from the function, which is the actual generator.\n",
        "text_in_bytes, sent = next(imdb())\n",
        "# To work with it, we have to decode the string, because it is in bytes format:\n",
        "decoded = text_in_bytes.decode('utf-8')\n",
        "# To avoid index errors in formatting:\n",
        "max_len = 100 if len(decoded) > 100 else len(decoded)-1\n",
        "print(\"The text, up to 100 characters, is:\\n{}\\nThe sentiment is: {}\".format(decoded[:max_len], \"\\033[92mPositive\" if sent else \"\\033[91mNegative\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7gpPVcgp7pN"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "As I mentioned, Trax allows for easy to use preprocessing pipelines.\n",
        "\n",
        "To do it, we implement a special type of object: the Serial Layer (for data). We can import it from trax.data.Serial, and this one allows us to process data one function at a time, in a sequential manner.\n",
        "\n",
        "```python\n",
        "# Remember that we imported trax.data as data\n",
        "pipeline = data.Serial(\n",
        "    function1(Usually loads data),\n",
        "    function2(does some preprocessing)...\n",
        ") -> returns a generator\n",
        "```\n",
        "\n",
        "We can also make a preprocessing pipeline and then feed the data generator to it. This way, we can do many important tasks, such as:\n",
        "\n",
        "```python\n",
        "my_data_generator = dataset_generator #function with yield statement\n",
        "preprocessing_pipeline = data.Serial(\n",
        "    my_remove_punct_function(puncts_to_remove='.,:;'),\n",
        "    my_stopword_removal_function(stopword_dict='/data/stopword_dict.json'),\n",
        "    my_tokenizer() ...\n",
        ")\n",
        "my_pipeline_generator = preprocessing_pipeline(my_data_generator())\n",
        "```\n",
        "Just remember that each of your functions takes as input the same format as the output of the previous function (eg.: (1) takes a string and returns a list; (2) takes a list and returns a list, etc.)\n",
        "\n",
        "(generator_sample)->(1)->(2)->...->(ml_output)\n",
        "\n",
        "For you to 'plug' your pipeline into Trax algorithms, the expected format is batch_input, batch_expected_output, mask weights (if any).\n",
        "\n",
        "In the following example, we show a pipeline that makes use of some useful preprocessing steps provided by Trax itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IhaEZycoE_G"
      },
      "source": [
        "pipeline_with_data = data.Serial(\n",
        "  data.TFDS('imdb_reviews', keys=('text', 'label'), train=True), #Loads dataset generator, you can use it inside the pipeline or outside, as we'll see an example below.\n",
        "  data.Tokenize(vocab_file='en_8k.subword', keys=[0]), #Tokenizes using a subword dict, it breaks words into common prefixes and suffixes for better generalization (useful for machine learning only).\n",
        "  data.Shuffle(), # Randomizes to avoid bias in the way data is displayed.\n",
        "  data.FilterByLength(max_length=128, length_keys=[0]), # This makes sure no sentence is bigger than 128 tokens, useful for some models that have size constraints.\n",
        "  data.BucketByLength(boundaries=[  32, 64, 128], # Bucketing is useful for batch training, it optimizes algorithm learning using batches of same-size sentences together.\n",
        "                      batch_sizes=[128,  64, 32],\n",
        "                      length_keys=[0]),\n",
        "  data.AddLossWeights() # Adds Loss Weights, useful in case of masking and padding, else padding can affect model loss.\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHPKYBX2tOa7"
      },
      "source": [
        "# Testing the pipeline:\n",
        "batch_token_ids, batch_target_values, batch_loss_weights = next(pipeline_with_data())\n",
        "# Here's how to detokenize the sentence input:\n",
        "first_tokenized_sentence = batch_token_ids[0][:10] #Using 10 max lenght is safe for our batches will have at least 32 len according to bucketing policy, even if some of them are just padding values.\n",
        "detokenized = data.detokenize(first_tokenized_sentence, vocab_file='en_8k.subword')\n",
        "print(\"Example sentence first 10 tokens: {}...\\nExample first 10 tokens after detokenization: {}...\\nSentiment: {}\".format(first_tokenized_sentence, \n",
        "                                                                                    detokenized, \n",
        "                                                                                    \"Positive\" if batch_target_values[0] else \"Negative\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSPmn3K_3kqt"
      },
      "source": [
        "# Here's an example with our pipeline\n",
        "\n",
        "def mock_generate_samples(mock_data):\n",
        "    \"\"\"\n",
        "    #Change this to use your file! (format: each line has text \\t sent)\n",
        "    with open(file) as f:\n",
        "        line = f.read()\n",
        "        for line in f:\n",
        "            values = line.split('\\t')\n",
        "            if len(values) < 2: \n",
        "                continue\n",
        "            yield values[0], values[1]\n",
        "        \"\"\"\n",
        "    strings = mock_data.split('.')\n",
        "    for string in strings:\n",
        "        values = string.split(';')\n",
        "        if len(values) < 2:\n",
        "            continue\n",
        "        yield values[0], values[1]\n",
        "\n",
        "# Here's an example into how to implement your own functions to the Pipeline.\n",
        "\n",
        "def change_all_to_lowercase(generator):\n",
        "    for entry in generator:\n",
        "        next_text, next_sent = entry\n",
        "        lowercased_text = next_text.lower()\n",
        "        yield lowercased_text, next_sent\n",
        "\n",
        "# You need to create a wrapper for it.\n",
        "\n",
        "def Change_All_To_LowerCase(): \n",
        "  \"\"\"Returns function to lowercase the sentence.\"\"\"\n",
        "  return lambda g: change_all_to_lowercase(g)\n",
        "\n",
        "my_generator = mock_generate_samples(\"This is a positive sentence; 1. This is another positive sentence; 1. This is a negative sentence :(; 0.\")\n",
        "my_pipeline = data.Serial(\n",
        "    change_all_to_lowercase, #Order matters! Lowercase before tokenizing.\n",
        "    data.Tokenize(vocab_file='en_8k.subword', keys=[0]),\n",
        "    data.Shuffle(),\n",
        "    data.AddLossWeights()\n",
        ")\n",
        "# We'll get a warning because our dataset is small, but it works.\n",
        "# We only iterate over results for simplicity. No need for that. We actually use the pipeline as input\n",
        "for tokens, sent, weights in my_pipeline(my_generator):\n",
        "    print(tokens, sent, data.detokenize(tokens, vocab_file='en_8k.subword'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFFM1ECXvfGO"
      },
      "source": [
        "## Creating Models\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Now that we've seen preprocessing, it's time to move into Modeling itself.\n",
        "\n",
        "Trax allows the use of models in two ways:\n",
        "\n",
        "<ul>\n",
        "<li>Predefined models</li>\n",
        "<ul>\n",
        "<li>Seq2Seq with Attention</li>\n",
        "<li>BERT</li>\n",
        "<li>Transformer</li>\n",
        "<li>Reformer</li>\n",
        "</ul>\n",
        "<li>Self-made models with layers.</li>\n",
        "</ul>\n",
        "\n",
        "Lets peek into one of Trax [pre-made models](https://trax-ml.readthedocs.io/en/latest/trax.models.html), the LSTMSeq2SeqAttn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSgIYwqsBZMk"
      },
      "source": [
        "# First, I'll download a vocab file to be used as an example.\n",
        "import os.path\n",
        "if not os.path.isfile('/content/pt.wiki.bpe.vs10000.vocab'):\n",
        "    !wget https://nlp.h-its.org/bpemb/pt/pt.wiki.bpe.vs10000.vocab\n",
        "# Suppose we'll use this model to make a Neural Machine Translator from English to Portuguese with a Seq2Seq Model.\n",
        "# We can do that instantiating a predefined model LSTMSeq2SeqAttn, you can check all parameters available in the Trax link above.\n",
        "seq2seq_model = trax.models.LSTMSeq2SeqAttn(input_vocab_size=data.vocab_size(vocab_file='en_8k.subword'), # I'm using data.vocab_size to get the vocab size that is going to be used by the tokenizer\n",
        "                                            target_vocab_size=data.vocab_size(vocab_type='subword', vocab_file='pt.wiki.bpe.vs10000.vocab', vocab_dir='/content'))\n",
        "# We can peek its structure:\n",
        "print(seq2seq_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s45j75CVxV1Z"
      },
      "source": [
        "That's a lot of things, right? Each of these are one of the network **layers**.\n",
        "\n",
        "Layers are the LEGO blocks of Trax!\n",
        "\n",
        "<img src='https://cdn.britannica.com/48/182648-050-6C20C6AB/LEGO-bricks.jpg' height=300>\n",
        "\n",
        "With these layers we can create our own models as well.\n",
        "\n",
        "Each layer is a function (or a whole bunch of functions bundled together) that gets some input and returns output in the promised format.\n",
        "\n",
        "Layers can be combined using what are called \"combiner layers\". \n",
        "\n",
        "By default, there are 3 \"combiner\" layers: \"Serial\" (sequential model), \"Branch\" (parallel model) and \"Residual\" (kind of a merge between Serial and Branch).\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "# tl is common shortcut for trax.layers, as np is for numpy and pd is for pandas.\n",
        "model = tl.Serial(\n",
        "    layer1,\n",
        "    layer2,\n",
        "    layer3\n",
        ")\n",
        "# Will run layer1>layer2>layer3\n",
        "model2 = tl.Branch(\n",
        "    layer1,\n",
        "    layer2\n",
        ")\n",
        "# Will run layer1 and layer2 at once.\n",
        "```\n",
        "\n",
        "### Create your own layers\n",
        "\n",
        "We can use trax [predefined layers](https://trax-ml.readthedocs.io/en/latest/trax.layers.html) (such as LSTM Cells, Dense layers, etc), but we can even make our own layers. \n",
        "\n",
        "There are two ways to create our own layers: the first is to implement it completely by inheriting from any layer (and overriding inputs). We'll see an example with a Dense Layer modification.\n",
        "\n",
        "The other method is to provide a function to ```trax.layers.PureLayer(forward_fn = your_function_here)``` and add it as a layer.\n",
        "\n",
        "For that, suppose we wanted to make a Dense Layer with a weird bias pattern in forward pass. We make a class that inherits from tl.Dense and modify the forward method, like such:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEoeaym13spV"
      },
      "source": [
        "from trax.fastmath import numpy as jnp #Numpy on steroids for the computations\n",
        "\n",
        "class MyDenseLayer(tl.Dense):\n",
        "    def forward(self, x):\n",
        "        \"\"\"Our modified forward pass with half bias term.\n",
        "        Args:\n",
        "        x: Tensor of same shape and dtype as the input signature used to\n",
        "            initialize this layer.\n",
        "        Returns:\n",
        "        Tensor of same shape and dtype as the input, except the final dimension\n",
        "        is the layer's `n_units` value.\n",
        "        \"\"\"\n",
        "        if self._use_bias:\n",
        "            if not isinstance(self.weights, (tuple, list)):\n",
        "                raise ValueError(f'Weights should be a (w, b) tuple or list; '\n",
        "                                f'instead got: {self.weights}')\n",
        "            w, b = self.weights\n",
        "            return jnp.dot(x, w) + b*0.5  # Here's where we add our modification, by halving the bias term. You can check the original source at https://github.com/google/trax/blob/master/trax/layers/core.py\n",
        "        else:\n",
        "            w = self.weights\n",
        "            return jnp.dot(x, w)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLFjscR-5Lc5"
      },
      "source": [
        "We can also define a full layer from scratch by inheriting from trax.base.Layer, but I won't get into that.\n",
        "\n",
        "Let us now build an entire model with this layer that we've built. We'll use a Serial Layer as a combiner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmgWTWsx2fnd"
      },
      "source": [
        "# Btw this is the same as the sample from trax quickstart guide.\n",
        "sentiment_analysis_model_sample = tl.Serial(\n",
        "    tl.Embedding(data.vocab_size(vocab_file='en_8k.subword'), d_feature=256), #Add an embeddings layer to turn tokens into embeddings with 256 dim and vocab size equal to the one used for tokenization.\n",
        "    tl.Mean(axis=1),  # Average on axis 1 (length of sentence).\n",
        "    MyDenseLayer(2),      # Classify 2 classes.\n",
        "    tl.LogSoftmax()   # Produce log-probabilities.\n",
        ")\n",
        "# Let us peek into our model\n",
        "print(sentiment_analysis_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGlvs7Z-7N3S"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Training Models\n",
        "\n",
        "So we already seen the way pipelines are created and how to create/use models.\n",
        "\n",
        "Now, it is time for us to do the trick and put both of them to work together.\n",
        "\n",
        "Let us put the machine to learn!\n",
        "\n",
        "<img src='https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1257&q=80' height=300>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Remember, there are two main methods of Machine Learning:\n",
        "\n",
        "*   Supervised\n",
        "*   unsupervised\n",
        "\n",
        "\n",
        "We're only covering supervised and self-supervised models (those where we get the targets from the inputs).\n",
        "\n",
        "To do Supervised Training in Trax, we have to define three important 'blocks':\n",
        "\n",
        "1.   The training task, which takes as input the labeled data, the loss layer, the optimizer and the number of steps between checkpoints.\n",
        "2.   The eval task, which takes as input the labeled data, the metrics and the number of eval batches.\n",
        "3.   The training loop, which puts all of this together.\n",
        "\n",
        "We get all that from trax.supervised. \n",
        "\n",
        "Before, though, let us use those pipelines we've learned before to build the labeled that for training and eval.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjTIMr5hBZc0"
      },
      "source": [
        "# First we get the streams from TFDS\n",
        "train_stream = trax.data.TFDS('imdb_reviews', keys=('text', 'label'), train=True)()\n",
        "eval_stream = trax.data.TFDS('imdb_reviews', keys=('text', 'label'), train=False)()\n",
        "\n",
        "# Next, we build the pipeline\n",
        "data_pipeline = trax.data.Serial(\n",
        "    trax.data.Tokenize(vocab_file='en_8k.subword', keys=[0]),\n",
        "    trax.data.Shuffle(),\n",
        "    trax.data.FilterByLength(max_length=2048, length_keys=[0]),\n",
        "    trax.data.BucketByLength(boundaries=[  32, 128, 512, 2048],\n",
        "                             batch_sizes=[512, 128,  32,    8, 1],\n",
        "                             length_keys=[0]),\n",
        "    trax.data.AddLossWeights()\n",
        "  )\n",
        "\n",
        "# Finally, we get the generators\n",
        "train_batches_stream = data_pipeline(train_stream)\n",
        "eval_batches_stream = data_pipeline(eval_stream)\n",
        "\n",
        "# We should redefine the model with the correct Dense layer, otherwise weird things will happen!\n",
        "sentiment_analysis_model = tl.Serial(\n",
        "    tl.Embedding(data.vocab_size(vocab_file='en_8k.subword'), d_feature=256),\n",
        "    tl.Mean(axis=1),\n",
        "    tl.Dense(2),\n",
        "    tl.LogSoftmax()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYEDkYjQ-j4E"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Now we can define the training/eval tasks and the loop.\n",
        "\n",
        "Just some important notes before.\n",
        "\n",
        "About the training and eval tasks:\n",
        "* We're using CrossEntropyLoss for the loss layer.\n",
        "* We're using Adam as the optimizer. This is the actual de-facto standard optimizer for DNNs.\n",
        "* We're also using accuracy as a metric. There are others which are fit to other situations. Check them here: https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.metrics\n",
        "\n",
        "About the Traning loop:\n",
        "* We create an output dir for the weights and checkpoints.\n",
        "* We plug the model and the train/eval tasks.\n",
        "* We run it by using training_loop.run =)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb1xK-HZ-QPA"
      },
      "source": [
        "from trax.supervised import training\n",
        "import os\n",
        "\n",
        "# Training task.\n",
        "train_task = training.TrainTask(\n",
        "    labeled_data=train_batches_stream,\n",
        "    loss_layer=tl.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adam(0.01),\n",
        "    n_steps_per_checkpoint=200, #This will print the results at every 200 training steps.\n",
        ")\n",
        "\n",
        "# Evaluaton task.\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data=eval_batches_stream,\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        "    n_eval_batches=20  # For less variance in eval numbers.\n",
        ")\n",
        "\n",
        "# Training loop saves checkpoints to output_dir.\n",
        "output_dir = os.path.expanduser('~/output_dir/')\n",
        "!rm -rf {output_dir}\n",
        "training_loop = training.Loop(sentiment_analysis_model,\n",
        "                              train_task,\n",
        "                              eval_tasks=[eval_task],\n",
        "                              output_dir=output_dir)\n",
        "\n",
        "# Run 2000 steps (batches).\n",
        "training_loop.run(2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4lROxqmAkcT"
      },
      "source": [
        "~85% accuracy for a simple model! Not too shabby, right?\n",
        "\n",
        "Okay, now how to use it?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Predicting from new inputs\n",
        "\n",
        "So we got a trained model, how do we use it?\n",
        "\n",
        "Simple! **Just feed a tokenized input to the model**!\n",
        "\n",
        "But, a caution note before: Trax models (as all current deep learning frameworks) expects the input to come with a batch dimmension besides the expected input dimmensions. So we have to wrap our sample arround that.\n",
        "\n",
        "Let us try it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa1ZqpjEkMyn"
      },
      "source": [
        "import numpy\n",
        "example_input = \"I loved the way that the actors were cast, also, It is clear that they've put a huge effort in post-production.\"\n",
        "#example_input = \"Very bad movie\"\n",
        "# Steps explained: \n",
        "# 1st: tokenize input. We cast it to an iterator to fake a generator.\n",
        "input_iter = iter([example_input])\n",
        "input_tokens = data.tokenize(input_iter, vocab_file='en_8k.subword')\n",
        "# 2nd: cast the results to a list and get the first value (the tokens, not the label or anything else)\n",
        "tokenized_input = list(input_tokens)[0]\n",
        "# 3rd: Add fake batch dimmension\n",
        "tokenized_with_batch = tokenized_input[None, :]\n",
        "# 4th: input it to the model and get the logprobs\n",
        "sentiment_log_probs = sentiment_analysis_model(tokenized_with_batch)\n",
        "# 5th: get the values to between 0 and 1 by exponentiating the logprobs.\n",
        "norm_log_probs = numpy.exp(sentiment_log_probs)\n",
        "# 6th: get if it's either true or false by checking the position of the greatest values at norm_log_probs dimmension 0 (that's what argmax does)\n",
        "sentiment = numpy.argmax(norm_log_probs[0])\n",
        "print('Input review:\\n\"{}\"\\nThe sentiment is: {}'.format(example_input, \"\\033[92mPositive\" if sentiment else \"\\033[91mNegative\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujbdHgv1JI_f"
      },
      "source": [
        "# Just a hint: make your life easier with a function:\n",
        "import numpy\n",
        "def parse_sentiment(text, model, vocab_file = 'en_8k.subword'):\n",
        "    input_iter = iter([example_input])\n",
        "    input_tokens = data.tokenize(input_iter, vocab_file=vocab_file)\n",
        "    tokenized_input = list(input_tokens)[0]\n",
        "    tokenized_with_batch = tokenized_input[None, :]\n",
        "    sentiment_log_probs = model(tokenized_with_batch)\n",
        "    norm_log_probs = numpy.exp(sentiment_log_probs)\n",
        "    sentiment = numpy.argmax(norm_log_probs[0])\n",
        "    return \"Positive\" if sentiment else \"Negative\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L08jLVygETPW"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Restoring the Model\n",
        "\n",
        "This enables us to retake training from a certain point, which is useful if you want to train for a really long time, if for some reason the running session crashes or if you want to test new parameters to the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi9kP_hcEYZ9"
      },
      "source": [
        "# This loads a checkpoint:\n",
        "training_loop.load_checkpoint(directory='~/output_dir/', filename=\"model.pkl.gz\")\n",
        "# Continue training:\n",
        "training_loop.run(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtKMdHa_OGMY"
      },
      "source": [
        "We can also load a pretrained model. This allows us to use models that have been trained before and are to be used in production:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLSyFS5KFO9_"
      },
      "source": [
        "# First, we need the same structure:\n",
        "new_model = tl.Serial(\n",
        "    tl.Embedding(data.vocab_size(vocab_file='en_8k.subword'), d_feature=256),\n",
        "    tl.Mean(axis=1),\n",
        "    tl.Dense(2),\n",
        "    tl.LogSoftmax()\n",
        ")\n",
        "# Then, we load the weights:\n",
        "new_model.init_from_file(file_name=\"/root/output_dir/model.pkl.gz\", weights_only=True) # Only load weights\n",
        "# Same result as before (I used a helper function for simplicity)\n",
        "print(\"The sentiment is: \", parse_sentiment(\"Very bad movie\", new_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSHmTbrISuCb"
      },
      "source": [
        "### Saving as Keras model and exporting to Tensorflow Serving\r\n",
        "\r\n",
        "As mentioned, Trax models can be easily converted to Keras layers. This also happens to entire models that can be warpped arround Keras layers and then be saved using the later to be exported to Tensorflow Serving. This is a convenient features that can help bringing your trained models into production.\r\n",
        "\r\n",
        "However, for that to happen, there's the need for the backend to be set to tensorflow-numpy, which is the same behind Keras. And **the model has to be trained using tensorflow-numpy** as backend (you cannot train with Jax and deploy with tensorflow-numpy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JrsiWBqaejv"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "# Setting the backend\r\n",
        "trax.fastmath.set_backend(\"tensorflow-numpy\")\r\n",
        "print(trax.fastmath.backend_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYi4mcMrdFHm"
      },
      "source": [
        "One hint: train the bulk on Jax (if the promise of efficiency keeps up) and then load tensorflow-numpy for a few epochs to convert the model to tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaerlTawUYh7"
      },
      "source": [
        "training_loop = training.Loop(sentiment_analysis_model,\r\n",
        "                              train_task,\r\n",
        "                              eval_tasks=[eval_task],\r\n",
        "                              output_dir=output_dir)\r\n",
        "\r\n",
        "# Run 2000 steps (batches).\r\n",
        "training_loop.run(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAUq0-flppto"
      },
      "source": [
        "To convert the model to a Keras layer, it is very straightforward: just call trax.AsKeras(model) and you get a keras layer as the output!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mEKbR60e6Sl"
      },
      "source": [
        "# To convert the model to Keras, simply run:\r\n",
        "keras_layer = trax.AsKeras(sentiment_analysis_model)\r\n",
        "# This will be a trax.trax2keras.AsKeras object\r\n",
        "print(keras_layer)\r\n",
        "\r\n",
        "# Run the Keras layer to verify it returns the same result.\r\n",
        "example_input = list(data.tokenize(iter([\"I loved the way that the actors were cast, also, It is clear that they've put a huge effort in post-production.\"]), vocab_file=\"en_8k.subword\"))[0]\r\n",
        "sentiment_activations = keras_layer(example_input[None, :])\r\n",
        "print(f'Keras returned sentiment activations: {numpy.asarray(sentiment_activations)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsxdt06wqDxf"
      },
      "source": [
        "Finally, use it as a layer on a full keras model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozt4Z2vqp6vu"
      },
      "source": [
        "# Create a full Keras  model using the layer from Trax.\r\n",
        "inputs = tf.keras.Input(shape=(None,), dtype='int32')\r\n",
        "hidden = keras_layer(inputs)\r\n",
        "# You can add other Keras layers here operating on hidden.\r\n",
        "outputs = hidden\r\n",
        "keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n",
        "print(keras_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FAc-2zCqKlH"
      },
      "source": [
        "To save the model to deployment, it is the same as in other Keras models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rju8xXEOqOfO"
      },
      "source": [
        "model_file = os.path.join(\"/content/\", \"saved_model\")\r\n",
        "# This yields a .pb file in the defined path: /content/saved_model/saved_model.pb\r\n",
        "keras_model.save(model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STw9Ad6dNtQ0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Other NLP Tasks with Trax\n",
        "\n",
        "Now that we've seen how to do a basic sentiment analysis with Trax, I'll give some examples on how to do other tasks as well.\n",
        "\n",
        "For some of them, we'll need quite a lot of data, so bear with me during the downloads, ok?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoxLzDZbeWQ8"
      },
      "source": [
        "---\r\n",
        "## Neural Machine Translation with Transformer model\r\n",
        "\r\n",
        "Suppose we wanted to train a transformer to translate from English to Portuguese. We could do that! First, we get some data, for example, the enpt pairs from the para_crawl corpus.\r\n",
        "\r\n",
        "Btw.: This is a huge corpus (about 2.65 gb after uncompression), so it takes a good while to download (more than 30 min)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcuWYTWAJBQW"
      },
      "source": [
        "nmt_train_stream = data.TFDS('para_crawl/enpt', keys=['en', 'pt'], train = True, eval_holdout_size=0.2)()\n",
        "nmt_eval_stream = data.TFDS('para_crawl/enpt', keys=['en', 'pt'], train = False, eval_holdout_size=0.2)()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucKu3jiWRBzj"
      },
      "source": [
        "### Data preparation and Model creation\r\n",
        "\r\n",
        "To train a NMT model, we need a vocab file that (preferrably) encompasses both languages at once. I added an example below, and a predefinde one. \r\n",
        "\r\n",
        "If you want to use it for your own tasks, feel free to change the line numbers (let the whole corpus be loaded to a file) and increase the vocab_size. Just remember to save this vocab file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qixfut5PiSmh"
      },
      "source": [
        "# This downloads a pretrained sentencepiece vocab that I pretrained, comment it to let it be generated.\n",
        "\n",
        "!wget https://storage.googleapis.com/dl_models/enpt_32k.model\n",
        "\n",
        "# The following steps will train a sentencepiece model for portuguese vocab. It is not necessary if you already have one.\n",
        "import os.path\n",
        "import io\n",
        "import sentencepiece as spm\n",
        "if not os.path.isfile('/content/enpt_32k.model'):\n",
        "    if not os.path.isfile('/content/lines.txt'):\n",
        "        def turn_port_to_file(stream):\n",
        "            linenum = 0\n",
        "            with open('lines.txt', 'w') as file:\n",
        "                for en, pt in stream:\n",
        "                    file.write(pt.decode('utf8')+\"\\n\")\n",
        "                    file.write(en.decode('utf8')+\"\\n\")\n",
        "                    linenum+=2\n",
        "                    if linenum > 100000:\n",
        "                        break\n",
        "        turn_port_to_file(nmt_train_stream)\n",
        "    model = io.BytesIO()\n",
        "    # We're using Sentencepiece with bpe model type. It will make a 32k words vocab with both portuguese and english worrds.\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input='/content/lines.txt', model_writer=model, vocab_size=32000, pad_id=0, unk_id=2,\n",
        "         bos_id=3, eos_id=1, pad_piece='<pad>', unk_piece='<unk>', bos_piece='<bos>', eos_piece='<eos>', num_threads=128, model_type='BPE') \n",
        "    # Important: Remember to use these pad, unk, bos and eos ids in custom vocab files, since Trax will expect them in this order.\n",
        "\n",
        "    # Serialize the model as file.\n",
        "    with open('/content/enpt_32k.model', 'wb') as f:\n",
        "        f.write(model.getvalue())\n",
        "\n",
        "# We set a simple model. Be aware that this is not the most efficient out there!\n",
        "transformer_nmt = trax.models.Transformer(input_vocab_size=data.vocab_size(vocab_type='sentencepiece', vocab_file='enpt_32k.model', vocab_dir='/content'),\n",
        "                                            output_vocab_size=data.vocab_size(vocab_type='sentencepiece', vocab_file='enpt_32k.model', vocab_dir='/content'),d_model=512,\n",
        "            n_encoder_layers=2,\n",
        "            n_decoder_layers=2,\n",
        "            n_heads =4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBbWXrLKiehv"
      },
      "source": [
        "# Prepare the preprocessing pipeline\n",
        "nmt_data_pipeline = trax.data.Serial(\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece', vocab_file='enpt_32k.model', vocab_dir='/content', keys=[0, 1]),\n",
        "    trax.data.Shuffle(),\n",
        "    trax.data.FilterByLength(max_length=1024, length_keys=[0, 1]),\n",
        "    trax.data.BucketByLength(boundaries=[16, 32, 64, 128, 256, 512, 1024], #Let us go only with smaller strings for simplicity\n",
        "                             batch_sizes=[512, 256, 128, 64, 32, 16, 8],\n",
        "                             length_keys=[0, 1]),\n",
        "    trax.data.AddLossWeights()\n",
        "  )\n",
        "\n",
        "nmt_train_batches_stream = nmt_data_pipeline(nmt_train_stream)\n",
        "nmt_eval_batches_stream = nmt_data_pipeline(nmt_eval_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dlD_jdpVrEg"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHNUQ_Ymjxpc"
      },
      "source": [
        "from trax.supervised import training\n",
        "import os\n",
        "\n",
        "# Training task, as usual, but with one more step: setting up the lr scale with warmup for better training.\n",
        "nmt_train_task = training.TrainTask(\n",
        "    labeled_data=nmt_train_batches_stream,\n",
        "    loss_layer=tl.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adam(0.1),\n",
        "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n",
        "    n_steps_per_checkpoint=200\n",
        ")\n",
        "\n",
        "# Evaluaton task.\n",
        "nmt_eval_task = training.EvalTask(\n",
        "    labeled_data=nmt_eval_batches_stream,\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        "    n_eval_batches=20\n",
        ")\n",
        "\n",
        "# Training loop saves checkpoints to output_dir.\n",
        "output_dir = os.path.expanduser('/content/nmt')\n",
        "nmt_training_loop = training.Loop(transformer_nmt,\n",
        "                              nmt_train_task,\n",
        "                              eval_tasks=[nmt_eval_task],\n",
        "                              output_dir=output_dir)\n",
        "#Here you can download a pretrained model. I've trained it for more than 50k steps, but got to just 55%.\n",
        "# !wget --directory-prefix=nmt/ https://storage.googleapis.com/dl_models/model.pkl.gz\n",
        "if os.path.isfile('/content/nmt/model.pkl.gz'):\n",
        "    nmt_training_loop.load_checkpoint('/content/nmt/model.pkl.gz')\n",
        "# This will take a while (I trained for about 30 hours!) It can get to about 56% accuracy. This is a vague metric, but the thing is it will be awful (will produce blabberish). \n",
        "# Unless you're like google and have 'unlimited' TPU's, you won't be able to do anything useful.\n",
        "nmt_training_loop.run(15000) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnKHb2PDj3Q4"
      },
      "source": [
        "### Performing Translations\r\n",
        "There are different techniques to perform a translation using a NMT model. \r\n",
        "\r\n",
        "One method is to use **autoregressive sampling** (currently implemented in trax 1.36), which does translation based on each other output in a linear format.\r\n",
        "\r\n",
        "There's another method not currently available in the current release: BeamSearch. It is on some of trax prototypes, so expect this to be available soon. BeamSearch does several translations and choses the one with best scoring, so the performance is better than autoregressive sampling.\r\n",
        "\r\n",
        "A word of caution: If you're going to use the model that I've pretrained, it will be blabberish, nonsense. To really train a translator we'd need more than 30h on a single TPU to achieve something useful. But it is a head start. If you need it and have the budget, its not that hard to scale!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekVIPqiKSfv4"
      },
      "source": [
        "# We start by tokenizing the input sentence.\n",
        "sentence = 'this does not yield good results'\n",
        "tokenized = list(trax.data.tokenize\n",
        "                 (iter([sentence]),  # Operates on streams.\n",
        "                                    vocab_type='sentencepiece', vocab_file='enpt_32k.model', vocab_dir='/content'))[0]\n",
        "\n",
        "# We decode the result of feeding the input to the the Transformer using autoregressive sampling.\n",
        "tokenized = tokenized[None, :]  # Add batch dimension.\n",
        "tokenized_translation = trax.supervised.decoding.autoregressive_sample(\n",
        "    transformer_nmt, tokenized, temperature=0.2)  # Temperature allows to help avoid \"rigid\" translations. The closer to 1 = more diverse results.\n",
        "\n",
        "# We de-tokenize the results.\n",
        "tokenized_translation = tokenized_translation[0][:-1]  # Remove batch and EOS token.\n",
        "translation = trax.data.detokenize(tokenized_translation,\n",
        "                                   vocab_type='sentencepiece', vocab_file='enpt_32k.model', vocab_dir='/content')\n",
        "print(translation)\n",
        "# You might get a lot of hiphens and other stuff with the model. That's because it's too weak.\n",
        "# head over to https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html to find a working translator from English to German."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qReaFZoxBBvl"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "## Named Entity Recognition with Reformer model\r\n",
        "\r\n",
        "This example has been adapted from the original one from Trax git repository (https://github.com/google/trax/blob/master/trax/examples/NER_using_Reformer.ipynb)\r\n",
        "\r\n",
        "We'll be using the dataset provided at Kaggle in the following link:https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus?select=ner_dataset.csv\r\n",
        "\r\n",
        "I've provided a Google Cloud Storage option for easier usability.\r\n",
        "\r\n",
        "We'll download it and load to pandas to do some simple preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYMCkff9CbN6"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy\r\n",
        "import random as rnd # For using random functions\r\n",
        "!wget https://storage.googleapis.com/dl_models/ner_dataset.csv\r\n",
        "df = pd.read_csv(\"/content/ner_dataset.csv\",encoding = 'ISO-8859-1')\r\n",
        "df = df.fillna(method = 'ffill')\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4imfnmZq7R4"
      },
      "source": [
        "### Data Preprocessing\r\n",
        "\r\n",
        "The following cell performs all steps needed to preprocess the input to be used to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvshzEUUDeQU"
      },
      "source": [
        "# Creates a class to process the corpus.\r\n",
        "class Get_sentence(object):\r\n",
        "    def __init__(self, data):\r\n",
        "        self.n_sent = 1\r\n",
        "        self.data = data\r\n",
        "        # Use an aggregator function to merge together all words, pos and tags with the same sentence ID.\r\n",
        "        agg_func = lambda s:[(w,p,t) for w,p,t in zip(s[\"Word\"].values.tolist(),\r\n",
        "                                                     s[\"POS\"].values.tolist(),\r\n",
        "                                                     s[\"Tag\"].values.tolist())]\r\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\r\n",
        "        self.sentences = [s for s in self.grouped]\r\n",
        "\r\n",
        "# Generate the corpus object by instantiating it with the dataframe\r\n",
        "getter = Get_sentence(df)\r\n",
        "\r\n",
        "# Access the merged sentences with the class .sentence property.\r\n",
        "sentence = getter.sentences\r\n",
        "\r\n",
        "# Create a list of unique words\r\n",
        "words = list(set(df[\"Word\"].values))\r\n",
        "\r\n",
        "# Create a list of unique tags\r\n",
        "words_tag = list(set(df[\"Tag\"].values))\r\n",
        "\r\n",
        "# Create a word to idx dict.\r\n",
        "word_idx = {w : i+1 for i ,w in enumerate(words)}\r\n",
        "word_idx['<PAD>']=len(word_idx)\r\n",
        "\r\n",
        "# Create a tag to idx dict.\r\n",
        "tag_idx =  {t : i for i ,t in enumerate(words_tag)}\r\n",
        "\r\n",
        "# Create a generator function to be used in the data pipeline.\r\n",
        "def id_translator(sentences):\r\n",
        "    while True: # Simulates a loop arround batcher. No problem since we are shuffling using trax.\r\n",
        "        for sentence in sentences: # Loops arround sentences.\r\n",
        "            words = [word_idx[triplet[0]] for triplet in sentence]\r\n",
        "            tags = [tag_idx[triplet[2]] for triplet in sentence]\r\n",
        "            yield numpy.array([words, tags])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cP6AAsQsYZ6"
      },
      "source": [
        "In the original sample, a data generator was manually created. \r\n",
        "\r\n",
        "We will simplify that by using Trax's own resources:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkAW7720siGi"
      },
      "source": [
        "batch_size=128\r\n",
        "ner_pipeline = data.Serial(\r\n",
        "    id_translator, #No need to tokenize, since we did it using the id_translator.\r\n",
        "    data.Shuffle(),\r\n",
        "    data.BucketByLength(batch_sizes=[batch_size],boundaries=[1024]),\r\n",
        "    data.AddLossWeights(id_to_mask=word_idx['<PAD>'])\r\n",
        ")\r\n",
        "\r\n",
        "# Let us split the dataset into train and eval\r\n",
        "split_size = 0.2\r\n",
        "split_limit = len(sentence)-int(len(sentence)*split_size)\r\n",
        "sentences_train = sentence[:split_limit]\r\n",
        "sentences_eval = sentence[split_limit:]\r\n",
        "\r\n",
        "# We now create the pipeline using the train and eval datasets\r\n",
        "ner_train_pipeline = ner_pipeline(sentences_train)\r\n",
        "ner_eval_pipeline = ner_pipeline(sentences_eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InTdChpQtbMl"
      },
      "source": [
        "### Creating and training the Model\r\n",
        "\r\n",
        "We create the model by using the predefined Reformer and adding a Dense layer with the number of tags as the number of units and a logsoftmax layer.\r\n",
        "\r\n",
        "You could go further and add some dropouts to avoid overfitting, but I'll leave that as is.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChgGJ21dM1q2"
      },
      "source": [
        "# This will\r\n",
        "NERModel = tl.Serial(\r\n",
        "    trax.models.reformer.Reformer(len(word_idx), d_model=50, ff_activation=tl.LogSoftmax),\r\n",
        "    tl.Dense(len(words_tag)),\r\n",
        "    tl.LogSoftmax()\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEMSgg7vOH9S"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "ner_train_task = training.TrainTask(\r\n",
        "    ner_train_pipeline,  \r\n",
        "    loss_layer = tl.CrossEntropyLoss(),\r\n",
        "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(100, 0.001),\r\n",
        "    optimizer = trax.optimizers.Adam(0.005), \r\n",
        "    n_steps_per_checkpoint=200\r\n",
        ")\r\n",
        "\r\n",
        "ner_eval_task = training.EvalTask(\r\n",
        "    labeled_data = ner_eval_pipeline, \r\n",
        "    metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], \r\n",
        "    n_eval_batches = 20 \r\n",
        ")\r\n",
        "ner_output_dir = \"/content/ner\"\r\n",
        "os.path.expanduser(ner_output_dir)\r\n",
        "\r\n",
        "ner_training_loop = training.Loop(\r\n",
        "    NERModel, \r\n",
        "    ner_train_task, \r\n",
        "    eval_tasks = [ner_eval_task], \r\n",
        "    output_dir = ner_output_dir) \r\n",
        "ner_training_loop.load_checkpoint(ner_output_dir+\"/model.pkl.gz\")\r\n",
        "# You'll have to run for a while to get really acceptable results. Don't be fooled by high accuracy, \r\n",
        "# there's a huge disproportion between untagged words and tagged words.\r\n",
        "ner_training_loop.run(n_steps = 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quJAe_hwsa2t"
      },
      "source": [
        "ner_training_loop.run(n_steps = 2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2Va0T6gx1Tc"
      },
      "source": [
        "### Performing tagging on new data\r\n",
        "\r\n",
        "To perform tagging on new data, you could tokenize the inputs manually (I did it by splitting the text and converting tokens to indexes).\r\n",
        "\r\n",
        "Just be aware of out of vocabulary words. Since this is a simple example, we're not treating them.\r\n",
        "\r\n",
        "After tokenizing, feed into the autoregressive_sample and it will suggest the tags in a sequential manner. \r\n",
        "\r\n",
        "* Chances are that, if you did train for long enought or did not use a weighting scheme, you'll get awful results (the model will just throw O at all words and no named entity will be detected)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9FJo_xIw5OG"
      },
      "source": [
        "test_sentence = \"I went to Ohio to make a new certificate , its price was 300 USD . Then I moved myself to England .\"\r\n",
        "split_words = test_sentence.split()\r\n",
        "tokens = numpy.array([word_idx[word] for word in split_words])\r\n",
        "tokens_with_batch = tokens[None, :]\r\n",
        "tokenized_translation = trax.supervised.decoding.autoregressive_sample(\r\n",
        "    NERModel, tokens_with_batch, temperature=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLKh6mq4iRSL"
      },
      "source": [
        "for word, tag in zip(split_words, tokenized_translation[0][:len(split_words)]):\r\n",
        "    print(word, words_tag[tag])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}